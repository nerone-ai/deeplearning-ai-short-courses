{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Quantization Theory\n",
    "\n",
    "**Objective:**\n",
    "Understand linear quantization techniques and implement weight quantization for pretrained language models using Hugging Face’s quanto library. This tutorial reorganizes the original material, adds explanatory notes, illustrative examples, calibration strategies, links to SOTA papers, and expert tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Quantization\n",
    "\n",
    "Quantization maps a large set of continuous values to a smaller, discrete set. In deep learning, reducing bit-width of weights and activations (e.g., FP32 → INT8) saves memory and speeds up inference, typically with minimal accuracy degradation.\n",
    "\n",
    "**Why Quantize?**\n",
    "- **Memory savings:** 32-bit → 8-bit yields 4× reduction in model size.\n",
    "- **Compute efficiency:** Integer operations (INT8) are faster and more energy-efficient on many accelerators.\n",
    "- **Deployment:** Smaller models fit on edge devices and reduce bandwidth requirements.\n",
    "\n",
    "**Linear Quantization Overview:**\n",
    "1. **Range estimation:** Find tensor’s minimum $r_{\\min}$ and maximum $r_{\\max}$.  \n",
    "2. **Compute parameters:**  \n",
    "   $$s = \\frac{r_{\\max} - r_{\\min}}{q_{\\max} - q_{\\min}}, \\quad z = \\mathrm{round}\\bigl( -r_{\\min} / s\\bigr).$$\n",
    "3. **Quantize:**  \n",
    "   $$q = \\mathrm{clip}\\bigl(\\mathrm{round}(r / s) + z, \\; q_{\\min}, q_{\\max}\\bigr).$$\n",
    "4. **Dequantize:**  \n",
    "   $$\\hat r = s\\,(q - z).$$\n",
    "\n",
    "- Here, $[q_{\\min}, q_{\\max}]$ is typically $[-128, 127]$ for signed INT8.\n",
    "- **Quantization error** $\\hat r - r$ arises but can be mitigated with calibration and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale: 0.0078, Zero-point: 127\n"
     ]
    }
   ],
   "source": [
    "# Function to compute scale (s) and zero-point (z)\n",
    "def get_quant_params(r_min, r_max, q_min=-128, q_max=127):\n",
    "    \"\"\"\n",
    "    Returns scale s and zero-point z for mapping floats in [r_min, r_max]\n",
    "    to ints in [q_min, q_max].\n",
    "    \"\"\"\n",
    "    s = (r_max - r_min) / (q_max - q_min)\n",
    "    z = int(round(-r_min / s))\n",
    "    z = max(q_min, min(q_max, z))\n",
    "    return s, z\n",
    "\n",
    "# Example usage\n",
    "r_min, r_max = -1.0, 1.0\n",
    "scale, zero_point = get_quant_params(r_min, r_max)\n",
    "print(f\"Scale: {scale:.4f}, Zero-point: {zero_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [-1.         -0.77777778 -0.55555556 -0.33333333 -0.11111111  0.11111111\n",
      "  0.33333333  0.55555556  0.77777778  1.        ]\n",
      "Quantized: [ -1  28  56  84 113 127 127 127 127 127]\n",
      "Reconstructed: [-1.0039 -0.7765 -0.5569 -0.3373 -0.1098  0.      0.      0.      0.\n",
      "  0.    ]\n",
      "Reconstruction Error: [-0.0039  0.0013 -0.0013 -0.0039  0.0013 -0.1111 -0.3333 -0.5556 -0.7778\n",
      " -1.    ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def quantize_tensor(r, s, z, q_min=-128, q_max=127):\n",
    "    q = np.round(r / s) + z\n",
    "    return np.clip(q, q_min, q_max).astype(np.int8)\n",
    "\n",
    "def dequantize_tensor(q, s, z):\n",
    "    return s * (q.astype(np.int32) - z)\n",
    "\n",
    "# Synthetic example\n",
    "tensor = np.linspace(-1, 1, num=10)\n",
    "q_tensor = quantize_tensor(tensor, scale, zero_point)\n",
    "reconstructed = dequantize_tensor(q_tensor, scale, zero_point)\n",
    "\n",
    "print(\"Original:\", tensor)\n",
    "print(\"Quantized:\", q_tensor)\n",
    "print(\"Reconstructed:\", np.round(reconstructed, 4))\n",
    "print(\"Reconstruction Error:\", np.round(reconstructed - tensor, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 0. Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "#!pip install transformers==4.35.0\n",
    "#!pip install quanto==0.0.11\n",
    "#!pip install torch==2.1.1\n",
    "\n",
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports and helper setup\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from helper import compute_module_sizes\n",
    "import numpy as np\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model size: 9.23 GB\n"
     ]
    }
   ],
   "source": [
    "# Load model & tokenizer\n",
    "model_name = \"EleutherAI/pythia-410m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Compute FP32 model size\n",
    "total_bytes = sum(compute_module_sizes(model).values())\n",
    "print(f\"FP32 model size: {total_bytes / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weight quantization with `quanto`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 model size: 3.22 GB\n"
     ]
    }
   ],
   "source": [
    "from quanto import quantize, freeze\n",
    "\n",
    "# Apply static INT8 quantization to weights\n",
    "quantize(model, weights=torch.int8, activations=None)\n",
    "\n",
    "# Finalize quantized model (freeze modifies model in-place)\n",
    "freeze(model)\n",
    "qmodel = model\n",
    "\n",
    "# Compute size of quantized model\n",
    "total_q_bytes = sum(compute_module_sizes(qmodel).values())\n",
    "print(f\"INT8 model size: {total_q_bytes / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with smaller model as in L4 tutorial: FLAN-T5 small\n",
    "\n",
    "#!pip install sentencepiece==0.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 output: annie scott\n",
      "FP32 model size: 0.29 GB\n"
     ]
    }
   ],
   "source": [
    "# Try with smaller model as in L4 tutorial: FLAN-T5 small\n",
    "\n",
    "# Load Google FLAN-T5 Small (original tutorial model)\n",
    "import sentencepiece as spm \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from helper import compute_module_sizes\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Generate a sample output\n",
    "input_text = \"Hello, my name is \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "print(\"FP32 output:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Compute FP32 model size\n",
    "module_sizes = compute_module_sizes(model)\n",
    "print(f\"FP32 model size: {module_sizes[''] / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (k): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (v): QLinear(in_features=512, out_features=384, bias=False)\n",
      "              (o): QLinear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): QLinear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): QLinear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): QLinear(in_features=512, out_features=32128, bias=False)\n",
      ")\n",
      "INT8 model size: 0.66 GB\n"
     ]
    }
   ],
   "source": [
    "# Quantize Google FLAN-T5 Small weights to INT8\n",
    "from quanto import quantize, freeze\n",
    "import torch\n",
    "\n",
    "quantize(model, weights=torch.int8, activations=None)\n",
    "\n",
    "# Display model to confirm quantization wrappers\n",
    "print(model)\n",
    "\n",
    "# Finalize quantized model (freeze modifies in-place)\n",
    "freeze(model)\n",
    "qmodel = model\n",
    "\n",
    "# Compute quantized model size\n",
    "total_q_bytes = sum(compute_module_sizes(qmodel).values())\n",
    "print(f\"INT8 model size: {total_q_bytes / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 -> annie scott\n",
      "INT8 -> annie scott\n"
     ]
    }
   ],
   "source": [
    "# Compare text generation outputs\n",
    "\n",
    "def sample_text(mdl, prompt=\"Hello, my name is\", max_new_tokens=10):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    out = mdl.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"FP32 ->\", sample_text(model))\n",
    "print(\"INT8 ->\", sample_text(qmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation Quantization & Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 145 QLinear modules for activation quantization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated INT8 model size: 0.66 GB\n"
     ]
    }
   ],
   "source": [
    "# Activation quantization requires matching modules (e.g., nn.Linear).\n",
    "# FLAN-T5 uses custom blocks; activations may not be quantized by default.\n",
    "\n",
    "from transformers import T5ForConditionalGeneration\n",
    "# Reload FP32 model\n",
    "model_act = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Quantize both weights and activations\n",
    "quantize(model_act, weights=torch.int8, activations=torch.int8)\n",
    "\n",
    "# Verify which modules were wrapped\n",
    "from quanto.nn.qlinear import QLinear\n",
    "wrapped = [(n, m) for n, m in model_act.named_modules() if isinstance(m, QLinear)]\n",
    "if not wrapped:\n",
    "    print(\"No QLinear modules found. Weight+activation quantization skipped for custom layers.\")\n",
    "else:\n",
    "    print(f\"Found {len(wrapped)} QLinear modules for activation quantization.\")\n",
    "\n",
    "# Prepare texts and batch size for calibration\n",
    "texts = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")[\"text\"][:50]\n",
    "batch_size = 8\n",
    "\n",
    "# Calibration: run both encoder and decoder to record activations\n",
    "# We'll use input_ids as decoder_input_ids to satisfy forward requirements\n",
    "model_act.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        # Provide decoder_input_ids equal to input_ids for teacher forcing\n",
    "        batch_outputs = model_act(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            decoder_input_ids=inputs.input_ids\n",
    "        )\n",
    "\n",
    "# Freeze calibrated model\n",
    "freeze(model_act)\n",
    "qmodel_act = model_act\n",
    "\n",
    "# Compute size and sample generation\n",
    "size_act = sum(compute_module_sizes(qmodel_act).values()) / 1024**3\n",
    "print(f\"Calibrated INT8 model size: {size_act:.2f} GB\")\n",
    "# Use generate to produce text\n",
    "# print(\"Sample generation after activation quantization:\", sample_text(qmodel_act))\n",
    "# Note:\n",
    "# - We passed `decoder_input_ids` only for calibration, which may not fully capture decoder dynamics when generating text.\n",
    "# - Calibration on teacher-forced outputs can leave the model ill-prepared for free-form generation, leading to gibberish.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantization-Aware Training (QAT)\n",
    "Quantization-aware training interleaves quantization during training so that the model learns to adapt to lower precision.\n",
    "\n",
    "**Key concept:** Maintain two sets of weights:\n",
    "- **Float weights:** used for gradient updates.\n",
    "- **Fake-quantized weights:** used in forward pass to simulate INT8 behavior.\n",
    "\n",
    "During backprop, gradients flow through the fake quantization (“Straight-Through Estimator”) to update float weights.\n",
    "\n",
    "**High-level Steps:**\n",
    "1. Insert quant/dequant op wrappers around key layers.\n",
    "2. Train on task data; forward uses quantized weights, backward updates float weights.\n",
    "3. Export final weights by quantizing the trained float weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recent SOTA Quantization in LLMs\n",
    " \n",
    "| Method   | Bits | Key Idea                                    | Calibration? | Summary                                                                                         | Paper Link                                      |\n",
    "|----------|------|---------------------------------------------|--------------|-------------------------------------------------------------------------------------------------|--------------------------------------------------|\n",
    "| LLM.INT8 | 8    | Outlier-aware two-stage (detect & scale)    | No           | Detects and rescales rare large weights (outliers) to minimize distortion.                       | [LLM.INT8](https://arxiv.org/abs/2208.07339)      |\n",
    "| QLoRA    | 4    | LoRA adapters + QAT at 4-bit                | No           | Fine-tunes low-rank adapters on a quantized model to recover performance in 4-bit precision.      | [QLoRA](https://arxiv.org/abs/2305.14314)        |\n",
    "| AWQ      | 4    | Per-channel activation-aware scaling        | Yes          | Learns per-channel scales based on activation statistics, improving INT4/INT8 accuracy.          | [AWQ](https://arxiv.org/abs/2306.00978)         |\n",
    "| GPTQ     | 4    | Hessian-aware greedy rounding               | No           | Uses Hessian information to guide greedy weight rounding, preserving model loss surface.          | [GPTQ](https://arxiv.org/abs/2210.17323)     |\n",
    "| HQQ      | 2    | Hybrid quant + learned reconstruction       | Yes          | Combines coarse quantization with reconstruction layers to achieve robust 2-bit representation. | [HQQ](https://mobiusml.github.io/hqq_blog/)         |\n",
    "| QuIP     | 2    | Importance-based pruning + quantization     | Yes          | Prunes negligible weights based on importance, then applies quantization to remaining ones.       | [QuIP](https://arxiv.org/abs/2307.13304)        |\n",
    "\n",
    "> **Tip:** Many of these methods include open-source implementations—start by experimenting on small models to familiarize yourself with their pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion & References\n",
    "\n",
    "**Key takeaways:**\n",
    "- Linear quantization (FP32→INT8) yields substantial memory and compute gains with simple formulas.\n",
    "- `quanto` automates weight quantization; calibration further refines activations.\n",
    "- QAT integrates quantization into training to reduce error.\n",
    "- SOTA LLM quantization spans 8-bit to 2-bit, balancing size vs. accuracy.\n",
    "\n",
    "**References:**\n",
    "1. T. Dettmers, \"LLM.INT8: Outlier-aware Quantization\" (Aug 2022)\n",
    "2. T. Dettmers, \"QLoRA: 4-bit LoRA Fine-tuning\" (May 2023)\n",
    "3. R. Fan et al., \"AWQ: Activation-Aware Quantization\" (2024)\n",
    "4. A. Frantar et al., \"GPTQ: Optimal Brain Quantization\" (2023)\n",
    "5. H. Badri et al., \"HQQ: Hybrid Quantization with Reconstruction\" (Nov 2023)\n",
    "6. R. Tseng et al., \"QuIP: Quantization via Importance-based Pruning\" (Jul 2023)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
