{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization Fundamentals – **Lesson 3**\n",
    "# Loading Models in Different Data Types\n",
    "\n",
    "> **Goal:** Explore how casting a model’s parameters to lower-precision dtypes (FP16, BF16) affects inference on CPU/GPU, and learn safe workflows to inspect & convert models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 0 · Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we’re using a torch version that supports BF16 on CPU\n",
    "#!pip install --quiet torch==2.1.1\n",
    "#!pip install --quiet transformers pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "\n",
    "import requests, math, gc\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 · Dummy Model Definition  \n",
    "*Replace / modify this section with your own architecture as needed.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paste or edit the DummyModel definition here\n",
    "class DummyModel(nn.Module):\n",
    "  \"\"\"\n",
    "  A dummy model that consists of an embedding layer\n",
    "  with two blocks of a linear layer followed by a layer\n",
    "  norm layer.\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    self.token_embedding = nn.Embedding(2, 2)\n",
    "\n",
    "    # Block 1\n",
    "    self.linear_1 = nn.Linear(2, 2)\n",
    "    self.layernorm_1 = nn.LayerNorm(2)\n",
    "\n",
    "    # Block 2\n",
    "    self.linear_2 = nn.Linear(2, 2)\n",
    "    self.layernorm_2 = nn.LayerNorm(2)\n",
    "\n",
    "    self.head = nn.Linear(2, 2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    hidden_states = self.token_embedding(x)\n",
    "\n",
    "    # Block 1\n",
    "    hidden_states = self.linear_1(hidden_states)\n",
    "    hidden_states = self.layernorm_1(hidden_states)\n",
    "\n",
    "    # Block 2\n",
    "    hidden_states = self.linear_2(hidden_states)\n",
    "    hidden_states = self.layernorm_2(hidden_states)\n",
    "\n",
    "    logits = self.head(hidden_states)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DummyModel()  # baseline FP32 model\n",
    "display(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 · Utility: Inspect Parameter dtypes \n",
    "**Observation:** By default, PyTorch creates all learnable parameters in **`torch.float32`** (32‑bit single precision), which offers ~7 decimal digits of precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_dtype(m):\n",
    "    \"\"\"Print each parameter name and its torch.dtype.\"\"\"\n",
    "    for name, param in m.named_parameters():\n",
    "        print(f\"{name:<25}  →  {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FP32 (default) model parameter dtypes:\\n\")\n",
    "print_param_dtype(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 · Casting to **FP16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp16 = DummyModel().half()  # quick cast helper\n",
    "print(\"\\nAfter .half():\")\n",
    "print_param_dtype(model_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1 · Inference Test\n",
    "dummy_input = torch.LongTensor([[1, 0], [0, 1]])\n",
    "\n",
    "# FP32 reference output\n",
    "logits_fp32 = model(dummy_input)\n",
    "print(\"FP32 logits:\\n\", logits_fp32)\n",
    "\n",
    "# FP16 forward pass (CPU)\n",
    "try:\n",
    "    logits_fp16 = model_fp16(dummy_input)\n",
    "except Exception as e:\n",
    "    print(\"\\033[91m\", type(e).__name__, \":\", e, \"\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why it fails:** Many CPU kernels (e.g. `aten::embedding`) don’t have **Half** (FP16) support. FP16 is primarily intended for **NVIDIA GPUs** (Tensor Cores) or recent chips.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 · Casting to **BF16** instead\n",
    "BF16 keeps the *same 8‑bit exponent* as FP32 → **wide numeric range**, but only 7‑bit mantissa → lower precision. Crucially, recent CPUs (AVX‑512 / AMX) and GPUs (Ampere+) often support BF16 kernels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4.1 · Deep‑copy then cast\n",
    "model_bf16 = deepcopy(model).to(torch.bfloat16)\n",
    "print(\"BF16 parameter dtypes:\\n\")\n",
    "print_param_dtype(model_bf16)\n",
    "\n",
    "\n",
    "### 4.2 · BF16 inference\n",
    "logits_bf16 = model_bf16(dummy_input)\n",
    "print(\"BF16 logits:\\n\", logits_bf16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 · Numerical Difference vs FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diff = torch.abs(logits_bf16 - logits_fp32).mean().item()\n",
    "max_diff  = torch.abs(logits_bf16 - logits_fp32).max().item()\n",
    "print(f\"Mean diff : {mean_diff:.3e} | Max diff : {max_diff:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice the error is usually **< 1e‑3**, negligible for many tasks – making BF16 a *drop‑in replacement* on supported hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros & Cons of Model Down‑Casting\n",
    "\n",
    "|  | **FP32** | **BF16** | **FP16** |\n",
    "|---|---|---|---|\n",
    "| Memory/Speed | Baseline | ≈50 % smaller, faster on BF16‑capable HW | ≈50 % smaller, *much* faster on GPUs with TensorCores |\n",
    "| Range | 8‑bit exponent | *Same* 8‑bit exponent (≈3.4e38) | 5‑bit exponent (≈6.5e4) |\n",
    "| Precision | 23‑bit mantissa | 7‑bit mantissa | 10‑bit mantissa |\n",
    "| CPU support | ✅ | ✅ (new CPUs) | ❌ many ops missing |\n",
    "| GPU support | ✅ | ✅ (Ampere+) | ✅ (Pascal+ with TensorCores) |\n",
    "\n",
    "**Guideline:**\n",
    "- **Use BF16** for quick wins on modern CPUs / GPUs without rewriting code.\n",
    "- **Use FP16** mainly on NVIDIA GPUs when kernels exist.\n",
    "\n",
    "\n",
    "1. Casting a model to a lower‑precision dtype is *one line*, but **hardware kernels must exist** for all ops.\n",
    "2. **FP16 on CPU will crash/raise** for many layers → prefer BF16 or keep FP32.\n",
    "3. Numerical error between BF16 and FP32 is often minimal (see diff above).\n",
    "4. Always verify model accuracy after down‑casting, especially on tasks with tight error budgets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 · Using Generative Models in Different Data Types\n",
    "\n",
    "**Focus:** Measure memory savings and qualitative accuracy when loading a large vision–language model from Hugging Face (BLIP image captioner) in **FP32 vs BF16**. Learn how `torch_dtype` and `torch.set_default_dtype()` control precision at load time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_param_dtype_lines(model, max_lines=None):\n",
    "    \"\"\"\n",
    "    Print the dtype of each parameter.\n",
    "    If max_lines is set, show only the first `max_lines`.\n",
    "    \"\"\"\n",
    "    iterator = model.named_parameters()\n",
    "    if max_lines is not None:\n",
    "        iterator = islice(iterator, max_lines)\n",
    "\n",
    "    shown = 0\n",
    "    for name, param in iterator:\n",
    "        print(f\"{name:<35} → {param.dtype}\")\n",
    "        shown += 1\n",
    "\n",
    "    total = sum(1 for _ in model.parameters())\n",
    "    if max_lines is not None and total > shown:\n",
    "        print(f\"... ({total - shown} more parameters not shown)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose & Load the Model (FP32)\n",
    "MODEL_ID = \"Salesforce/blip-image-captioning-base\"\n",
    "\n",
    "# Load full‑precision model & processor\n",
    "auto_processor = BlipProcessor.from_pretrained(MODEL_ID)\n",
    "model_fp32     = BlipForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "\n",
    "### Inspect parameter dtypes & memory footprint\n",
    "print(\"FP32 parameter dtypes:\\n\")\n",
    "print_param_dtype_lines(model_fp32, max_lines=10)\n",
    "\n",
    "fp32_bytes = model_fp32.get_memory_footprint()\n",
    "print(f\"\\nMemory footprint (FP32): {fp32_bytes/1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hugging Face models default to **float32** for maximum accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the Same Model Directly in **BF16**\n",
    "model_bf16 = BlipForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"BF16 parameter dtypes:\\n\")\n",
    "print_param_dtype_lines(model_bf16, max_lines=10)\n",
    "\n",
    "bf16_bytes = model_bf16.get_memory_footprint()\n",
    "print(f\"\\nMemory footprint (BF16): {bf16_bytes/1e6:.1f} MB\")\n",
    "print(f\"Relative size vs FP32 : {bf16_bytes/fp32_bytes:.2f}×\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BF16 halves the memory requirement (one byte per value saved) while keeping the *same numeric range* as FP32.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 · Caption an Image – Qualitative Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to load an image from URL (or local file) - provided in course assets\n",
    "def load_image(img_url):\n",
    "    image = Image.open(requests.get(\n",
    "        img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "    return image\n",
    "\n",
    "img_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"\n",
    "image   = load_image(img_url).convert(\"RGB\")\n",
    "image.resize((500,350))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate with FP32\n",
    "inputs = auto_processor(images=image, text=\"a picture of\", return_tensors=\"pt\")\n",
    "output_ids = model_fp32.generate(**inputs, max_new_tokens=20)\n",
    "caption_fp32 = auto_processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"FP32 caption →\", caption_fp32)\n",
    "\n",
    "### Generate with BF16\n",
    "# same inputs, but forward on bf16 model (weights already bf16; inputs stay fp32)\n",
    "with torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n",
    "    output_ids_bf16 = model_bf16.generate(**inputs, max_new_tokens=20)\n",
    "caption_bf16 = auto_processor.decode(output_ids_bf16[0], skip_special_tokens=True)\n",
    "print(\"BF16 caption →\", caption_bf16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** Captions are identical or almost identical! Autoregressive accumulation error is minimal because BF16 keeps the 8‑bit exponent; only mantissa precision drops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 · Controlling the *Global* Default dtype\n",
    "Sometimes you’d like *every module you instantiate* to come up in lower precision without passing `torch_dtype=` everywhere.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.bfloat16)\n",
    "\n",
    "# any new layers / models created *after* this line inherit BF16\n",
    "bf16_dummy = DummyModel()\n",
    "print_param_dtype_lines(bf16_dummy, max_lines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always restore default to FP32 afterwards to avoid surprises\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best practice:** Set‑and‑reset inside a `with`‑context or script section; don’t leave global default at BF16 for unrelated code.\n",
    "\n",
    "**Takeaways**\n",
    "1. **torch_dtype= parameter** lets you load Hugging Face models directly in lower precision — faster and lighter than casting afterwards.\n",
    "2. **BF16 ≈ 50 % memory cut** with negligible accuracy loss for vision‑language tasks, thanks to its full FP32 exponent range.\n",
    "3. FP16 kernels are plentiful on GPUs; BF16 kernels increasingly so on both CPUs & GPUs (Ampere+ / Sapphire Rapids). On unsupported hardware, PyTorch falls back to FP32.\n",
    "4. **torch.set_default_dtype() changes the dtype only for *new tensors and layers you create afterward*. It doesn’t shrink the checkpoint you download; pretrained weights are still cast locally. Use it when *initializing* fresh models, but prefer the **torch_dtype=** flag (or dtype‑specific checkpoints) when loading Hugging Face models to save RAM right away.\n",
    "\n",
    "**Practical recommendations**\n",
    "1. Loading pretrained models: pass **torch_dtype=torch.bfloat16** (or float16) to from_pretrained.\n",
    "2. Building new modules from scratch: temporarily call **torch.set_default_dtype(desired_dtype)** then reset so the rest of your code isn’t surprised.\n",
    "3. Casting after load **(model.half())**: fine for small models or quick tests, but avoid for multi-GB checkpoints on limited RAM/VRAM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_quantization)",
   "language": "python",
   "name": "dl_quantization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
