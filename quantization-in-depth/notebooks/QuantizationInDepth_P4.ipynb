{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4181672e",
   "metadata": {},
   "source": [
    "# Quantization in Depth - Part 4\n",
    "\n",
    "**Weight Packing and LLM Quantization Challenges**\n",
    "\n",
    "This final lesson in the Hugging Face quantization series covers the practical and theoretical limitations of ultra-low-bit quantization (e.g., 2-bit and 4-bit) and challenges in applying quantization to large language models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f4d498",
   "metadata": {},
   "source": [
    "## 1. Why Weight Packing is Necessary\n",
    "\n",
    "PyTorch does not currently support native 2-bit or 4-bit tensors:\n",
    "\n",
    "```python\n",
    "torch.tensor([0, 1], dtype=torch.int4)  # ❌ Not supported\n",
    "```\n",
    "\n",
    "In practice, we must store these quantized values in `uint8` tensors (8 bits), which can introduce overhead if not handled correctly. To truly benefit from quantization, we need to **pack multiple low-bit weights into a single byte**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2d66849",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'int4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint4\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/DL_quantization/lib/python3.9/site-packages/torch/__init__.py:1833\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 1833\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'int4'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "torch.tensor([0,1], dtype=torch.int4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3abdbfe",
   "metadata": {},
   "source": [
    "## 2. Manual Weight Packing (2-bit)\n",
    "\n",
    "To store four 2-bit weights in a single `uint8`, we manually shift and OR their bits together.\n",
    "\n",
    "### Example:\n",
    "Given: `[1, 0, 3, 2]` with 2-bit quantization, we want to pack into one byte:\n",
    "```\n",
    "From 0000 0001 - 0000 0000 - 0000 0011 - 0000 0010\n",
    "To Binary:  01 00 11 10  =>  10110001 => 177\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca80f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packed: tensor([177], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def pack_weights(uint8tensor, bits):\n",
    "    \"\"\"\n",
    "    Packs a tensor of low-bit values into a uint8 tensor.\n",
    "\n",
    "    Args:\n",
    "        uint8tensor (torch.Tensor): 1D tensor of integers (e.g., values between 0-3 for 2-bit).\n",
    "        bits (int): Number of bits per weight (e.g., 2 or 4).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Packed tensor of type uint8.\n",
    "    \"\"\"\n",
    "    # Check if the input length is a multiple of 8 // bits\n",
    "    if uint8tensor.shape[0] * bits % 8 != 0:\n",
    "        raise ValueError(f\"The input length must be a multiple of {8 // bits}\")\n",
    "\n",
    "    # Calculate the number of values and steps\n",
    "    num_values = uint8tensor.shape[0] * bits // 8\n",
    "    # Number of steps per value\n",
    "    num_steps = 8 // bits\n",
    "\n",
    "    # Initialize the packed tensor\n",
    "    packed_tensor = torch.zeros((num_values,), dtype=torch.uint8)\n",
    "    unpacked_idx = 0\n",
    "\n",
    "    # Pack the weights\n",
    "    for i in range(num_values):\n",
    "        for j in range(num_steps):\n",
    "            # Shift the current value to the correct position and OR it with the packed tensor\n",
    "            packed_tensor[i] |= uint8tensor[unpacked_idx] << (bits * j)\n",
    "            # Move to the next value    \n",
    "            unpacked_idx += 1\n",
    "\n",
    "    return packed_tensor\n",
    "\n",
    "# Test the function\n",
    "unpacked = torch.tensor([1, 0, 3, 2], dtype=torch.uint8)\n",
    "packed = pack_weights(unpacked, 2)\n",
    "print(\"Packed:\", packed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae91e9c",
   "metadata": {},
   "source": [
    "## 3. Manual Weight Unpacking (2-bit)\n",
    "\n",
    "To use the packed weights in operations, we need to unpack them back into individual low-bit values.\n",
    "We apply **bit-shifting** and a **bitmask** to recover the original values.\n",
    "\n",
    "The bitmask for 2-bit is:\n",
    "$$\n",
    "\\text{mask} = 2^\\text{bits} - 1 = 3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0657469a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacked: tensor([1, 0, 3, 2], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "def unpack_weights(uint8tensor, bits):\n",
    "    \"\"\"\n",
    "    Unpacks a tensor of packed uint8 values into original low-bit weights.\n",
    "\n",
    "    Args:\n",
    "        uint8tensor (torch.Tensor): Packed uint8 tensor.\n",
    "        bits (int): Number of bits per weight.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Unpacked weights as uint8.\n",
    "    \"\"\"\n",
    "    # Number of values for each uint8\n",
    "    num_values = uint8tensor.shape[0] * 8 // bits\n",
    "    # Number of steps per value\n",
    "    num_steps = 8 // bits\n",
    "    # Initialize the unpacked tensor\n",
    "    unpacked_tensor = torch.zeros((num_values,), dtype=torch.uint8)\n",
    "\n",
    "    # Create a mask for the bits we want to extract\n",
    "    mask = 2 ** bits - 1\n",
    "    unpacked_idx = 0\n",
    "\n",
    "    # Unpack the weights\n",
    "    for i in range(uint8tensor.shape[0]):\n",
    "        for j in range(num_steps):\n",
    "            # Shift the current value to the correct position \n",
    "            val = uint8tensor[i] >> (bits * j)\n",
    "            # Extract the bits we want\n",
    "            unpacked_tensor[unpacked_idx] = val & mask\n",
    "            # Move to the next value    \n",
    "            unpacked_idx += 1\n",
    "\n",
    "    return unpacked_tensor\n",
    "\n",
    "# Test unpacking\n",
    "recovered = unpack_weights(torch.tensor([177], dtype=torch.uint8), 2)\n",
    "print(\"Unpacked:\", recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecf211c",
   "metadata": {},
   "source": [
    "## 4. Why Quantizing LLMs is Hard\n",
    "\n",
    "Quantizing LLMs introduces unique challenges due to **emergent outlier features** at scale. As shown in the [LLM.int8](https://arxiv.org/abs/2208.07339) paper, outliers appear in more layers and a larger portion of the sequence space as models grow.\n",
    "\n",
    "This causes linear quantization to break down:\n",
    "\n",
    "- **Outliers dominate the quantization range**, wasting bit precision for typical values.\n",
    "- **Final predictions are sensitive to errors**, especially in autoregressive settings.\n",
    "\n",
    "To address this, recent research proposes **outlier-aware** methods like LLM.INT8, SmoothQuant, GPTQ, AWQ, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15af6535",
   "metadata": {},
   "source": [
    "## 5. Recent SOTA Quantization in LLMs\n",
    "\n",
    "| Method   | Bits | Key Idea                                    | Calibration? | Summary                                                                                         | Paper Link                                      |\n",
    "|----------|------|---------------------------------------------|--------------|-------------------------------------------------------------------------------------------------|--------------------------------------------------|\n",
    "| LLM.INT8 | 8    | Outlier-aware two-stage (detect & scale)    | No           | Detects and rescales rare large weights (outliers) to minimize distortion.                       | [LLM.INT8](https://arxiv.org/abs/2208.07339)      |\n",
    "| QLoRA    | 4    | LoRA adapters + QAT at 4-bit                | No           | Fine-tunes low-rank adapters on a quantized model to recover performance in 4-bit precision.      | [QLoRA](https://arxiv.org/abs/2305.14314)        |\n",
    "| AWQ      | 4    | Per-channel activation-aware scaling        | Yes          | Learns per-channel scales based on activation statistics, improving INT4/INT8 accuracy.          | [AWQ](https://arxiv.org/abs/2306.00978)         |\n",
    "| GPTQ     | 4    | Hessian-aware greedy rounding               | No           | Uses Hessian information to guide greedy weight rounding, preserving model loss surface.          | [GPTQ](https://arxiv.org/abs/2210.17323)     |\n",
    "| HQQ      | 2    | Hybrid quant + learned reconstruction       | Yes          | Combines coarse quantization with reconstruction layers to achieve robust 2-bit representation. | [HQQ](https://mobiusml.github.io/hqq_blog/)         |\n",
    "| QuIP     | 2    | Importance-based pruning + quantization     | Yes          | Prunes negligible weights based on importance, then applies quantization to remaining ones.       | [QuIP](https://arxiv.org/abs/2307.13304)        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce76205",
   "metadata": {},
   "source": [
    "Quantizing large language models (LLMs) requires much more than simply converting weights to int8 or int4. These models exhibit **emergent behaviors** at scale — including the appearance of **activation outliers**, which break the assumptions behind naive uniform quantization.\n",
    "\n",
    "---\n",
    "\n",
    "#### The Outlier Problem in LLMs (From LLM.int8 Paper)\n",
    "\n",
    "A critical insight from [LLM.int8 (Dettmers et al., 2022)](https://arxiv.org/abs/2208.07339) is that **outliers — extremely large activation values — appear increasingly in deeper layers and across many sequence dimensions** as model size grows.\n",
    "\n",
    "> **Key figure:** The paper shows that for models like OPT-175B:\n",
    "> - Outliers are present in **all layers**\n",
    "> - Over **75% of the sequence dimensions** are affected\n",
    "> - The effect becomes worse as **C4 perplexity improves** (i.e., more capable models)\n",
    "\n",
    "This phenomenon makes classical linear quantization **infeasible**, as the quantization range becomes dominated by a small number of large values, wasting bit precision on normal features.\n",
    "\n",
    "---\n",
    "\n",
    "#### LLM.int8 (2022)\n",
    "\n",
    "**Key idea:** Separate weight matrix multiplication into two components:\n",
    "1. **Main weights:** quantized normally (INT8)\n",
    "2. **Outlier rows/columns:** kept in higher precision (e.g. FP16)\n",
    "\n",
    "This is done by detecting high-magnitude weights/activations and **processing them outside of quantization**. The approach allows:\n",
    "- High accuracy preservation\n",
    "- Real-world deployment at 8-bit\n",
    "- No calibration required, simple two-stage implementation.\n",
    "\n",
    "---\n",
    "\n",
    "#### SmoothQuant (2022)\n",
    "\n",
    "**Problem:** Activations are harder to quantize than weights, due to outliers stretching their range.\n",
    "\n",
    "**Key idea:** Smooth the activation distributions by **migrating the variance from activations to weights**.\n",
    "\n",
    "Let:\n",
    "- \\( X \\) = activation\n",
    "- \\( W \\) = weight\n",
    "\n",
    "SmoothQuant introduces a **scaling factor \\( s \\)** and applies:\n",
    "$$\n",
    "\\\\tilde{X} = X / s \\\\quad \\\\text{and} \\\\quad \\\\tilde{W} = W \\\\cdot s\n",
    "$$\n",
    "\n",
    "This transformation leaves the output unchanged but **balances** the dynamic range, so both \\( \\\\tilde{X} \\) and \\( \\\\tilde{W} \\) are easier to quantize.\n",
    "\n",
    "- Works with **A8W8** quantization (both activations and weights in int8).\n",
    "- Requires **calibration dataset**.\n",
    "\n",
    "---\n",
    "\n",
    "#### AWQ (Activation-aware Weight Quantization, 2023)\n",
    "\n",
    "**Key idea:** LLM performance depends disproportionately on a small subset of weights (called **salient weights**). \n",
    "\n",
    "**How it works:**\n",
    "- Run a small calibration set\n",
    "- Determine which channels/layers produce **activation outliers**\n",
    "- Keep top ~1% of **salient weights in FP16**, quantize the rest\n",
    "\n",
    "Also applies **per-channel scaling** based on activation magnitude.\n",
    "\n",
    "- Produces **high accuracy** at 4-bit with **low overhead**  \n",
    "- Efficient for **deployment on edge devices**  \n",
    "- Compatible with many LLMs (OPT, LLaMA, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "> These methods show that successful quantization of LLMs is not only a matter of reducing bit width, but of **respecting the structure of information flow** within the network. Outlier handling, scaling, and weight importance must be taken into account.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1fde8",
   "metadata": {},
   "source": [
    "## 6. Conclusion & Further Resources\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Manual packing/unpacking is essential for true 2/4-bit quantization.\n",
    "- Quantizing LLMs is harder due to outlier sensitivity.\n",
    "- New methods address these challenges by selectively preserving precision.\n",
    "\n",
    "**Further Reading:**\n",
    "- [LLM.INT8](https://arxiv.org/abs/2208.07339)\n",
    "- [QLoRA](https://arxiv.org/abs/2305.14314)\n",
    "- [GPTQ](https://arxiv.org/abs/2210.17323)\n",
    "- [SmoothQuant](https://arxiv.org/abs/2211.10438)\n",
    "- [AWQ](https://arxiv.org/abs/2306.00978)\n",
    "- [QuIP](https://arxiv.org/abs/2307.13304)\n",
    "- [HQQ Blog](https://mobiusml.github.io/hqq_blog/)\n",
    "\n",
    "> Further reading: Check also the `llama.cpp` repo, MITHanLab resources (https://hanlab.mit.edu/ and their course \"TinyML and Efficient Deep Learning Computing\" https://hanlab.mit.edu/courses/2024-fall-65940 ), and HF quantization docs (https://huggingface.co/docs/transformers/quantization)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL_quantization)",
   "language": "python",
   "name": "dl_quantization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
